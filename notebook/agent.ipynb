{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (1.54.5)\n",
      "Requirement already satisfied: groq in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (1.0.1)\n",
      "Requirement already satisfied: pillow in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (11.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai groq python-dotenv pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/it/Desktop/proj_codingAgent/venv/lib/python3.13/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "import json\n",
    "from typing import List, Dict, Any, Callable\n",
    "import ast\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables via dotenv\n",
    "\n",
    "path = '../.env.local'\n",
    "load_dotenv(dotenv_path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/it/Desktop/proj_multiagent/venv/lib/python3.13/site-packages (11.0.0)\n",
      "Requirement already satisfied: pytesseract in /Users/it/Desktop/proj_multiagent/venv/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/it/Desktop/proj_multiagent/venv/lib/python3.13/site-packages (from pytesseract) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLMs\n",
    "groq = Groq(api_key=groq_api_key)\n",
    "openrouter = OpenAI(\n",
    "    base_url = 'https://openrouter.ai/api/v1',\n",
    "    api_key = openrouter_api_key\n",
    ")\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOURCE**:\n",
    "* [Github Repo](https://github.com/team-headstart/Agent-Workshop)\n",
    "\n",
    "To create our AI Agent, we will define the following functions:\n",
    "\n",
    "1. **Planner**: This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
    "\n",
    "2. **Reasoner**: This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
    "\n",
    "3. **Actioner**: Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
    "\n",
    "4. **Evaluator**: This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
    "\n",
    "5. **generate_and_execute_code**: This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
    "\n",
    "6. **executor**: Depending on the action decided by the “actioner,” this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
    "\n",
    "7. **final_answer_extractor**: After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
    "\n",
    "8. **autonomous_agent**: This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response.\n",
    "\n",
    "![](../public/images/digram.png)\n",
    "![](../public/images/workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(client, prompt, openai_model=\"gpt-4o-mini\", json_mode=False):\n",
    "    response = None  # Initialize response to avoid unbound variable errors\n",
    "\n",
    "    if client == \"openai\":\n",
    "        try:\n",
    "            kwargs = {\n",
    "                \"model\": openai_model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "            if json_mode:\n",
    "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "            response = openai.chat.completions.create(**kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with OpenAI client: {e}\")\n",
    "            raise e  # Propagate error\n",
    "\n",
    "    elif client == \"groq\":\n",
    "        models = [\n",
    "            \"llama-3.1-8b-instant\",\n",
    "            \"llama-3.1-70b-versatile\",\n",
    "            \"llama3-70b-8192\",\n",
    "            \"llama3-8b-8192\",\n",
    "            \"gemma2-9b-it\"\n",
    "        ]\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                kwargs = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "                }\n",
    "                if json_mode:\n",
    "                    kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "                response = groq.chat.completions.create(**kwargs)\n",
    "                break  # Exit loop on success\n",
    "            except Exception as e:\n",
    "                print(f\"Error with Groq model '{model}': {e}\")\n",
    "                continue\n",
    "\n",
    "        if response is None:  # All models failed\n",
    "            try:\n",
    "                kwargs = {\n",
    "                    \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "                }\n",
    "                if json_mode:\n",
    "                    kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "                response = openrouter.chat.completions.create(**kwargs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with fallback model: {e}\")\n",
    "                raise e  # Propagate error if fallback also fails\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid client: {client}\")\n",
    "\n",
    "    if response is None:\n",
    "        raise RuntimeError(\"No response was generated by any model.\")\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner(user_query) -> List[str]:\n",
    "    prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to answer the question.\n",
    "\n",
    "    Each subtask should be either a reasoning task or a code generation task. Never duplicate a task.\n",
    "\n",
    "    Here are the only 2 actions that can be taken for each subtask:\n",
    "    - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification\n",
    "    - reasoning: This action involves providing reasoning for what to do to complete the subtask\n",
    "\n",
    "    Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
    "\n",
    "    Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
    "\n",
    "    Return the result as a JSON list of strings, where each string is a subtask.\n",
    "\n",
    "    Here is an example JSON response:\n",
    "    {{\"subtasks\": [\"subtask1\", \"subtask2\", \"subtask3]}}\n",
    "\"\"\"\n",
    "    # Fetch response from LLM\n",
    "    data = get_llm_response('groq', prompt, json_mode=True)\n",
    "    \n",
    "    # Extract content from response\n",
    "    try:\n",
    "        # Parse the content into JSON\n",
    "        response = json.loads(data)\n",
    "        pprint({\"Parsed Response:\": response})\n",
    "        \n",
    "        # Return the subtasks\n",
    "        subtasks = response['subtasks']\n",
    "        print(subtasks)\n",
    "        return subtasks\n",
    "    except (KeyError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error while processing LLM response: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"gpt-4o-mini\"):\n",
    "    if reasoning_prompt:\n",
    "        # Reasoning prompt\n",
    "        prompt += f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
    "\n",
    "    openai_response = get_llm_response('openai', prompt, openai_model)\n",
    "    groq_response = get_llm_response('groq', prompt)\n",
    "\n",
    "    pprint(f'OpenAI Response: {openai_response}')\n",
    "    pprint(f'\\n\\nGroq Response: {groq_response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Parsed Response:': {'subtasks': ['reasoning: Extract the specific word of '\n",
      "                                   \"interest from the user's query\",\n",
      "                                   'generate_code: Count the occurrences of '\n",
      "                                   \"the letter 'r' in the word 'strawberry'\",\n",
      "                                   'reasoning: Return the count as the final '\n",
      "                                   'answer',\n",
      "                                   'generate_code: Create a Python function to '\n",
      "                                   'count occurrences of a letter in a word']}}\n",
      "[\"reasoning: Extract the specific word of interest from the user's query\", \"generate_code: Count the occurrences of the letter 'r' in the word 'strawberry'\", 'reasoning: Return the count as the final answer', 'generate_code: Create a Python function to count occurrences of a letter in a word']\n"
     ]
    }
   ],
   "source": [
    "query = \"How many r's are in the word 'strawberry' ?\"\n",
    "subtasks = planner(\"groq\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'OpenAI Response: The word \"strawberry\" contains 2 \\'r\\'s.'\n",
      "\"\\n\\nGroq Response: The word 'strawberry' has 2 'r's.\"\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(query, reasoning_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
    "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "\n",
    "    Here are all the subtasks to complete in order to answer the user's query:\n",
    "    <subtasks>\n",
    "    {json.dumps(subtasks)}\n",
    "    </subtasks>\n",
    "\n",
    "    Here is the short-term memory (result of previous subtasks):\n",
    "    \n",
    "        {json.dumps(memory)}\n",
    "\n",
    "    The current subtask to complete is:\n",
    "    <current_subtask>\n",
    "    {current_subtask}\n",
    "    </current_subtask>\n",
    "\n",
    "    - Provide concise reasoning on how to execute the current subtask, considering previous results and subtasks.\n",
    "    - Prioritize explicit details over assumed patterns.\n",
    "    - Avoid unnecessary complications in problem-solving.\n",
    "\n",
    "    Return the result as a JSON object with 'reasoning' as a key.\n",
    "\n",
    "    Example JSON response:\n",
    "    {{\n",
    "        \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = get_llm_response('groq', prompt, json_mode=True)\n",
    "    response_json = json.loads(response)\n",
    "    return response_json['reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "\n",
    "    The subtasks are:\n",
    "    <subtasks>\n",
    "    {json.dumps(subtasks)}\n",
    "    </subtasks>\n",
    "\n",
    "    The current subtask is:\n",
    "    <current_subtask>\n",
    "    {current_subtask}\n",
    "    </current_subtask>\n",
    "\n",
    "    The reasoning for this subtask is:\n",
    "    <reasoning>\n",
    "    {reasoning}\n",
    "    </reasoning>\n",
    "\n",
    "    Determine the most appropriate action to take:\n",
    "    - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
    "    - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
    "\n",
    "    Consider the overall goal and previous results when determining the action.\n",
    "\n",
    "    Return the result as a JSON object with 'action' and 'parameters' keys. \n",
    "    The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
    "\n",
    "    Example JSON responses:\n",
    "    {{\n",
    "        \"action\": \"generate_code\",\n",
    "        \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
    "    }}\n",
    "    {{\n",
    "        \"action\": \"reasoning\",\n",
    "        \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = get_llm_response('groq', prompt, json_mode=True)\n",
    "    response_json = json.loads(response)\n",
    "    return response_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    code_generation_prompt = f\"\"\"\n",
    "    Generate Python code to implement the following task: '{prompt}'\n",
    "\n",
    "    Here is the overall goal of answering the user's query: '{user_query}'\n",
    "\n",
    "    Keep in mind the results of the previous subtasks, adn use them to complete the current subtask.\n",
    "    <memory>\n",
    "    {json.dumps(memory)}\n",
    "    </memory>\n",
    "\n",
    "    Here are the guidelines for generating the code:\n",
    "    - Return only the Python code, without any explanations or markdown formatting.\n",
    "    - The code should always print or return a value.\n",
    "    - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
    "    - Do not ever use the input() function in your code, use defined values instead.\n",
    "    - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
    "    - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
    "    - Don't ever provide the execution result in your response, just give me the code.\n",
    "    - If your code needs to import any libraries, do it within the code itself.\n",
    "    - The code should be self-contained and ready to execute on its own.\n",
    "    - Prioritize explicit details over assumed patterns.\n",
    "    - Avoid unnecessary complications in problem-solving.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Generate the code using the LLM\n",
    "    generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
    "    print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
    "\n",
    "    # Step 2: Write the generated code to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\") as temp_file:\n",
    "        temp_file_path = temp_file.name\n",
    "        temp_file.write(generated_code.encode(\"utf-8\"))\n",
    "\n",
    "    try:\n",
    "        # Step 3: Execute the code in a sandboxed subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\", temp_file_path],  # Command to execute the Python file\n",
    "            capture_output=True,         # Capture stdout and stderr\n",
    "            text=True,                   # Decode output as text\n",
    "            timeout=5                    # Timeout in seconds to prevent infinite loops\n",
    "        )\n",
    "\n",
    "        # Step 4: Check the result\n",
    "        if result.returncode == 0:  # Successful execution\n",
    "            execution_output = result.stdout.strip()\n",
    "        else:  # Error during execution\n",
    "            execution_output = f\"Error: {result.stderr.strip()}\"\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        execution_output = \"Error: Code execution timed out.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        execution_output = f\"Error during execution: {str(e)}\"\n",
    "\n",
    "    finally:\n",
    "        # Step 5: Clean up the temporary file\n",
    "        if os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "    # Step 6: Return the result\n",
    "    return {\n",
    "        \"generated_code\": generated_code,\n",
    "        \"execution_result\": execution_output\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
    "    if action == \"generate_code\":\n",
    "        print(f\"Generating code for: {parameters['prompt']}\")\n",
    "        return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
    "    elif action == \"reasoning\":\n",
    "        return parameters[\"prompt\"]\n",
    "    else:\n",
    "        return f\"Action '{action}' not implemented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "\n",
    "    The subtasks to complete to answer the user's query are:\n",
    "    \n",
    "        {json.dumps(subtasks)}\n",
    "    \n",
    "\n",
    "    The current subtask to complete is:\n",
    "    \n",
    "        {current_subtask}\n",
    "    \n",
    "\n",
    "    The result of the current subtask is:\n",
    "    \n",
    "        {action_info}\n",
    "    \n",
    "\n",
    "    The execution result of the current subtask is:\n",
    "    \n",
    "        {execution_result}\n",
    "    \n",
    "\n",
    "    Here is the short-term memory (result of previous subtasks):\n",
    "    \n",
    "        {json.dumps(memory)}\n",
    "    \n",
    "\n",
    "    Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the overall query.\n",
    "\n",
    "    Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
    "\n",
    "    Example JSON response:\n",
    "    {{\n",
    "        \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
    "        \"retry\": false\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "    return response\n",
    "\n",
    "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
    "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "\n",
    "    The subtasks completed to answer the user's query are:\n",
    "    \n",
    "        {json.dumps(subtasks)}\n",
    "    \n",
    "\n",
    "    The memory of the thought process (short-term memory) is:\n",
    "    \n",
    "        {json.dumps(memory)}\n",
    "    \n",
    "\n",
    "    Extract the final answer that directly addresses the user's query, from the memory.\n",
    "    Provide only the essential information without unnecessary explanations.\n",
    "\n",
    "    Return a JSON object with 'finalAnswer' as a key.\n",
    "\n",
    "    Here is an example JSON response:\n",
    "    {{\n",
    "        \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "    return response[\"finalAnswer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
    "    memory = []\n",
    "    subtasks = planner(user_query)\n",
    "\n",
    "    print(\"User Query:\", user_query)\n",
    "    print(f\"Subtasks: {subtasks}\")\n",
    "\n",
    "    for subtask in subtasks:\n",
    "        max_retries = 1\n",
    "        for attempt in range(max_retries):\n",
    "\n",
    "            reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
    "            action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
    "\n",
    "            print(f\"\\n\\n ****** Action Info: {action_info} ****** \\n\\n\")\n",
    "\n",
    "            execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
    "\n",
    "            print(f\"\\n\\n ****** Execution Result: {execution_result} ****** \\n\\n\")\n",
    "            evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
    "\n",
    "            step = {\n",
    "                \"subtask\": subtask,\n",
    "                \"reasoning\": reasoning,\n",
    "                \"action\": action_info,\n",
    "                \"evaluation\": evaluation\n",
    "            }\n",
    "            memory.append(step)\n",
    "\n",
    "            print(f\"\\n\\nSTEP: {step}\\n\\n\")\n",
    "\n",
    "            if not evaluation[\"retry\"]:\n",
    "                break\n",
    "\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Max retries reached for subtask: {subtask}\")\n",
    "\n",
    "    final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Query # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surgeon is the boy's mother. The scenario highlights a common assumption that surgeons are male, but in this case, the surgeon is a woman.\n"
     ]
    }
   ],
   "source": [
    "query = \"The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\"\n",
    "result = get_llm_response(\"openai\", query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Parsed Response:': {'subtasks': ['reasoning: Identify the entities in the '\n",
      "                                   \"user's query, such as the surgeon and the \"\n",
      "                                   'boy.',\n",
      "                                   'reasoning: Determine the relationship '\n",
      "                                   'between the surgeon and the boy in the '\n",
      "                                   'given context.',\n",
      "                                   'reasoning: Use the identified relationship '\n",
      "                                   \"to determine the surgeon's role to the \"\n",
      "                                   'boy.',\n",
      "                                   'generate_code: Analyze the constructed '\n",
      "                                   'relationship to output the role of the '\n",
      "                                   'surgeon to the boy.']}}\n",
      "[\"reasoning: Identify the entities in the user's query, such as the surgeon and the boy.\", 'reasoning: Determine the relationship between the surgeon and the boy in the given context.', \"reasoning: Use the identified relationship to determine the surgeon's role to the boy.\", 'generate_code: Analyze the constructed relationship to output the role of the surgeon to the boy.']\n",
      "User Query: The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\n",
      "Subtasks: [\"reasoning: Identify the entities in the user's query, such as the surgeon and the boy.\", 'reasoning: Determine the relationship between the surgeon and the boy in the given context.', \"reasoning: Use the identified relationship to determine the surgeon's role to the boy.\", 'generate_code: Analyze the constructed relationship to output the role of the surgeon to the boy.']\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Break down the user's query into its primary components to identify the entities, such as extracting phrases like 'the surgeon', 'the boy', and '(the boy's) father'.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: Break down the user's query into its primary components to identify the entities, such as extracting phrases like 'the surgeon', 'the boy', and '(the boy's) father'. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': \"reasoning: Identify the entities in the user's query, such as the surgeon and the boy.\", 'reasoning': \"To identify the entities, scrutinize the user's query and break it down into its primary components. Extract phrases or words that signify individuals or objects, such as 'the surgeon', 'the boy', and '(the boy's) father'.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Break down the user's query into its primary components to identify the entities, such as extracting phrases like 'the surgeon', 'the boy', and '(the boy's) father'.\"}}, 'evaluation': {'evaluation': 'The result is a faithful reproduction of the subtask description, suggesting that a correct analysis has started.', 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Analyze the sentence 'the boy's father says, 'I can't operate on this boy, he's my son!' and the context to establish the relationship between the surgeon and the boy.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: Analyze the sentence 'the boy's father says, 'I can't operate on this boy, he's my son!' and the context to establish the relationship between the surgeon and the boy. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': 'reasoning: Determine the relationship between the surgeon and the boy in the given context.', 'reasoning': \"Analyze the words '(the boy's) father' and 'the surgeon' and their relationship to the boy. Consider the context where the surgeon says 'he's my son', indicating the surgeon is the boy's father.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Analyze the sentence 'the boy's father says, 'I can't operate on this boy, he's my son!' and the context to establish the relationship between the surgeon and the boy.\"}}, 'evaluation': {'evaluation': 'The result of analyzing the sentence and context establishes the relationship between the surgeon and the boy as parent and child.', 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Given the established parent-child relationship between the surgeon and the boy, use this relationship to determine the surgeon's role and confirm that the surgeon is the boy's father.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: Given the established parent-child relationship between the surgeon and the boy, use this relationship to determine the surgeon's role and confirm that the surgeon is the boy's father. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': \"reasoning: Use the identified relationship to determine the surgeon's role to the boy.\", 'reasoning': \"Given the established relationship between the surgeon and the boy as parent and child from the previous subtask, use the parent-child relationship to determine that the surgeon is the boy's father. This relationship indicates a biological or familial connection, which can be used to conclude the surgeon's role to the boy.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Given the established parent-child relationship between the surgeon and the boy, use this relationship to determine the surgeon's role and confirm that the surgeon is the boy's father.\"}}, 'evaluation': {'evaluation': \"The result effectively uses the established parent-child relationship to determine the surgeon's role as the boy's father, providing a reasonable and contextually accurate answer to the current subtask.\", 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"The surgeon's role to the boy is directly stated by their relationship, as they are the boy's father. Therefore, the surgeon's role to the boy is that of a parent.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: The surgeon's role to the boy is directly stated by their relationship, as they are the boy's father. Therefore, the surgeon's role to the boy is that of a parent. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': 'generate_code: Analyze the constructed relationship to output the role of the surgeon to the boy.', 'reasoning': \"To complete this subtask, analyze the identified relationship that the surgeon is the boy's father using the action of the surgeon, explicitly stating the surgeon's status as a parent, to directly conclude the surgeon's role to the boy in the context of the user's query. This determination can be expressed through a clear and concise statement within the generated output.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"The surgeon's role to the boy is directly stated by their relationship, as they are the boy's father. Therefore, the surgeon's role to the boy is that of a parent.\"}}, 'evaluation': {'evaluation': \"The result effectively uses the established relationship to output the role of the surgeon to the boy, confirming that the surgeon is the boy's parent.\", 'retry': False}}\n",
      "\n",
      "\n",
      "FINAL ANSWER:  The surgeon to the boy is the boy's father.\n"
     ]
    }
   ],
   "source": [
    "query = \"The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\"\n",
    "result = autonomous_agent(query)\n",
    "print(\"FINAL ANSWER: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Query # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bear was white. The only place on Earth where it's possible to travel 5 steps south, then 5 steps east, and finally 5 steps north and end up back at the original starting point (the tent) is at the North Pole. Since polar bears are the only bears that live in the Arctic region, and they have white fur, the bear inside the tent would be white.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The Bear Puzzle: A hunter leaves his tent. He travels 5 steps due south, 5 steps due east, and 5 steps due north. He arrives back at his tent, and sees a brown bear inside it. What color was the bear?\"\n",
    "\n",
    "result = get_llm_response(\"openai\", prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Parsed Response:': {'subtasks': ['reasoning: Understand the problem '\n",
      "                                   'statement and identify any assumptions',\n",
      "                                   'reasoning: Identify the implications of '\n",
      "                                   \"the hunter's movements on the bear's \"\n",
      "                                   'location',\n",
      "                                   'reasoning: Deduce the necessary conditions '\n",
      "                                   'for the bear to be in the tent when the '\n",
      "                                   'hunter returns',\n",
      "                                   'reasoning: Conclude the color of the bear '\n",
      "                                   'based on the absence of any relevant '\n",
      "                                   'information about its color in the '\n",
      "                                   'surroundings',\n",
      "                                   \"generate_code: Observe that the bear's \"\n",
      "                                   \"color was not affected by the hunter's \"\n",
      "                                   'movements, therefore, its color remains a '\n",
      "                                   'constant']}}\n",
      "['reasoning: Understand the problem statement and identify any assumptions', \"reasoning: Identify the implications of the hunter's movements on the bear's location\", 'reasoning: Deduce the necessary conditions for the bear to be in the tent when the hunter returns', 'reasoning: Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings', \"generate_code: Observe that the bear's color was not affected by the hunter's movements, therefore, its color remains a constant\"]\n",
      "User Query: The Bear Puzzle: A hunter leaves his tent. He travels 5 steps due south, 5 steps due east, and 5 steps due north. \n",
      "He arrives back at his tent, and sees a brown bear inside it. What color was the bear?\n",
      "Subtasks: ['reasoning: Understand the problem statement and identify any assumptions', \"reasoning: Identify the implications of the hunter's movements on the bear's location\", 'reasoning: Deduce the necessary conditions for the bear to be in the tent when the hunter returns', 'reasoning: Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings', \"generate_code: Observe that the bear's color was not affected by the hunter's movements, therefore, its color remains a constant\"]\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Read the problem statement carefully to grasp the sequence of the hunter's movements and identify the explicit details mentioned about the hunter's steps.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: Read the problem statement carefully to grasp the sequence of the hunter's movements and identify the explicit details mentioned about the hunter's steps. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': 'reasoning: Understand the problem statement and identify any assumptions', 'reasoning': \"Read the problem statement carefully to grasp the sequence of the hunter's movements. Make a list of explicit details mentioned in the problem, such as the hunter's steps south, east, and north.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Read the problem statement carefully to grasp the sequence of the hunter's movements and identify the explicit details mentioned about the hunter's steps.\"}}, 'evaluation': {'evaluation': \"The result is a reasonable answer for the current subtask. The subtask's prompt leads to a better understanding of the hunter's movements, and the result does not contain any explicitly identified assumptions which can now be completed in the next subtask.\", 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Analyze the sequence of the hunter's movements: 5 steps due south, 5 steps due east, and 5 steps due north. Visualize the possible locations of the bear in the tent when the hunter returns and draw a diagram to support your analysis.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: Analyze the sequence of the hunter's movements: 5 steps due south, 5 steps due east, and 5 steps due north. Visualize the possible locations of the bear in the tent when the hunter returns and draw a diagram to support your analysis. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': \"reasoning: Identify the implications of the hunter's movements on the bear's location\", 'reasoning': \"Analyze the hunter's movements in sequence, focusing on the bear's location relative to the hunter's starting and ending points. Based on the hunter's movements, draw a diagram to visualize the possible locations of the bear in the tent when the hunter returns.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Analyze the sequence of the hunter's movements: 5 steps due south, 5 steps due east, and 5 steps due north. Visualize the possible locations of the bear in the tent when the hunter returns and draw a diagram to support your analysis.\"}}, 'evaluation': {'evaluation': \"The result is a reasonable answer for the current subtask. The sequence of the hunter's movements has been analyzed and a thought process has been initiated for the visualization of possible bear locations in the tent.\", 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"The hunter moves 5 steps due south, then 5 steps due east, then 5 steps due north to return to his tent. Since the northward steps cancel out the southward steps, the bear will still be in the tent because the hunter's movements do not affect the relative locations of the two points (the hunter's current position and the bear's position in the tent).\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: The hunter moves 5 steps due south, then 5 steps due east, then 5 steps due north to return to his tent. Since the northward steps cancel out the southward steps, the bear will still be in the tent because the hunter's movements do not affect the relative locations of the two points (the hunter's current position and the bear's position in the tent). ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': 'reasoning: Deduce the necessary conditions for the bear to be in the tent when the hunter returns', 'reasoning': \"Deduce the necessary conditions for the bear to be in the tent when the hunter returns by considering the hunter's steps north. Think about the relationship between the hunter's ending point and the bear's location relative to their common starting point.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"The hunter moves 5 steps due south, then 5 steps due east, then 5 steps due north to return to his tent. Since the northward steps cancel out the southward steps, the bear will still be in the tent because the hunter's movements do not affect the relative locations of the two points (the hunter's current position and the bear's position in the tent).\"}}, 'evaluation': {'evaluation': \"The result is a reasonable answer for the current subtask. It correctly addresses the necessary conditions for the bear to be in the tent when the hunter returns, considering the hunter's movements and their implications on the bear's location.\", 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Finish the subtask 'Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings'. Since no relevant information about the bear's color is given, and the hunter's movements do not affect the bear's color, we can conclude that the bear's color remains a constant and we cannot derive its color from the given problem.\"}} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: Finish the subtask 'Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings'. Since no relevant information about the bear's color is given, and the hunter's movements do not affect the bear's color, we can conclude that the bear's color remains a constant and we cannot derive its color from the given problem. ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': 'reasoning: Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings', 'reasoning': \"Given that the problem does not provide any information about the bear's color, and the hunter's movements do not affect the bear's location, it can be inferred that the bear's color remains constant and is not related to its initial position in the tent. This conclusion can be drawn from the explicit information provided in the problem statement and the previous subtasks.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Finish the subtask 'Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings'. Since no relevant information about the bear's color is given, and the hunter's movements do not affect the bear's color, we can conclude that the bear's color remains a constant and we cannot derive its color from the given problem.\"}}, 'evaluation': {'evaluation': \"The result is a reasonable answer to the current subtask and makes sense in the context of the overall query. It correctly concludes that the bear's color remains a constant since no information about the bear's color is provided.\", 'retry': False}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Action Info: {'action': 'generate_code', 'parameters': {'prompt': 'Write a function that prints the color of the bear based on the constant conclusion.'}} ****** \n",
      "\n",
      "\n",
      "Generating code for: Write a function that prints the color of the bear based on the constant conclusion.\n",
      "\n",
      "\n",
      "Generated Code: start|print(\"The color of the bear is brown.\")|END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ****** Execution Result: {'generated_code': 'print(\"The color of the bear is brown.\")', 'execution_result': 'The color of the bear is brown.'} ****** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STEP: {'subtask': \"generate_code: Observe that the bear's color was not affected by the hunter's movements, therefore, its color remains a constant\", 'reasoning': \"The result to generate_code can be directly obtained from the final conclusion of the previous subtask 'reasoning: Conclude the color of the bear based on the absence of any relevant information about its color in the surroundings'. This conclusion explicitly stated that the bear's color remains a constant and is not affected by the hunter's movements.\", 'action': {'action': 'generate_code', 'parameters': {'prompt': 'Write a function that prints the color of the bear based on the constant conclusion.'}}, 'evaluation': {'evaluation': 'The result is a reasonable answer for the current subtask, and makes sense in the context of the overall query.', 'retry': False}}\n",
      "\n",
      "\n",
      "The color of the bear was not affected by the hunter's movements, therefore, its color remains a constant.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The Bear Puzzle: A hunter leaves his tent. He travels 5 steps due south, 5 steps due east, and 5 steps due north. \n",
    "He arrives back at his tent, and sees a brown bear inside it. What color was the bear?\"\"\"\n",
    "\n",
    "result = autonomous_agent(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
